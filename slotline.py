# -*- coding: utf-8 -*-
"""
Created on Thu Feb  9 19:53:28 2017

@author: bharathk
"""

import math
import random
import matplotlib.pyplot as plt
import time

import numpy as np
from sklearn.svm import LinearSVC
import slotline_plot
from sklearn.neural_network import MLPClassifier
from sklearn.neural_network import MLPRegressor
from sklearn import linear_model
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import SGDClassifier, Perceptron


class NN:
    def __init__(self, NI, NH, NO):
        # number of nodes in layers
        self.ni = NI + 1  # +1 for bias
        self.nh = NH
        self.no = NO

        # initialize node-activations
        self.ai, self.ah, self.ao = [], [], []
        self.ai = [1.0] * self.ni
        self.ah = [1.0] * self.nh
        self.ao = [1.0] * self.no

        # create node weight matrices
        self.wi = makeMatrix(self.ni, self.nh)
        self.wo = makeMatrix(self.nh, self.no)
        # initialize node weights to random vals
        randomizeMatrix(self.wi, -0.2, 0.2)
        randomizeMatrix(self.wo, -2.0, 2.0)
        # create last change in weights matrices for momentum
        self.ci = makeMatrix(self.ni, self.nh)
        self.co = makeMatrix(self.nh, self.no)

    def runNN(self, inputs):
        if len(inputs) != self.ni - 1:
            print('incorrect number of inputs')

        for i in range(self.ni - 1):
            self.ai[i] = inputs[i]

        for j in range(self.nh):
            sum = 0.0
            for i in range(self.ni):
                sum += (self.ai[i] * self.wi[i][j])
            self.ah[j] = sigmoid(sum)

        for k in range(self.no):
            sum = 0.0
            for j in range(self.nh):
                sum += (self.ah[j] * self.wo[j][k])
            self.ao[k] = sigmoid(sum)

        return self.ao

    def backPropagate(self, targets, N, M):

        # calc output deltas
        # we want to find the instantaneous rate of change of ( error with respect to weight from node j to node k)
        # output_delta is defined as an attribute of each ouput node. It is not the final rate we need.
        # To get the final rate we must multiply the delta by the activation of the hidden layer node in question.
        # This multiplication is done according to the chain rule as we are taking the derivative of the activation function
        # of the ouput node.
        # dE/dw[j][k] = (t[k] - ao[k]) * s'( SUM( w[j][k]*ah[j] ) ) * ah[j]
        output_deltas = [0.0] * self.no
        for k in range(self.no):
            error = targets[k] - self.ao[k]
            output_deltas[k] = error * dsigmoid(self.ao[k])

            # update output weights
        for j in range(self.nh):
            for k in range(self.no):
                # output_deltas[k] * self.ah[j] is the full derivative of dError/dweight[j][k]
                change = output_deltas[k] * self.ah[j]
                self.wo[j][k] += N * change + M * self.co[j][k]
                self.co[j][k] = change

        # calc hidden deltas
        hidden_deltas = [0.0] * self.nh
        for j in range(self.nh):
            error = 0.0
            for k in range(self.no):
                error += output_deltas[k] * self.wo[j][k]
            hidden_deltas[j] = error * dsigmoid(self.ah[j])

        # update input weights
        for i in range(self.ni):
            for j in range(self.nh):
                change = hidden_deltas[j] * self.ai[i]
                # print 'activation',self.ai[i],'synapse',i,j,'change',change
                self.wi[i][j] += N * change + M * self.ci[i][j]
                self.ci[i][j] = change

        # calc combined error
        # 1/2 for differential convenience & **2 for modulus
        error = 0.0
        for k in range(len(targets)):
            error = 0.5 * (targets[k] - self.ao[k]) ** 2
        return error

    def weights(self):
        print('Input weights:')
        for i in range(self.ni):
            print(self.wi[i])
        print
        print('Output weights:')
        for j in range(self.nh):
            print(self.wo[j])
        print('')

    def test(self, patterns):
        for p in patterns:
            inputs = p[0]
            self.runNN(inputs)
            # print ('Inputs:', p[0], '-->', self.runNN(inputs), ' Target', p[1])

    def train(self, patterns, max_iterations=1000, N=0.5, M=0.1):
        for i in range(max_iterations):
            for p in patterns:
                inputs = p[0]
                targets = p[1]
                self.runNN(inputs)
                error = self.backPropagate(targets, N, M)
                # if i % 50 == 0:
                #   print ('Combined error', error)
        self.test(patterns)

    def fun(self, x):
        return self.runNN(x)


def sigmoid(x):
    return math.tanh(x)


# the derivative of the sigmoid function in terms of output
# proof here:
# http://www.math10.com/en/algebra/hyperbolic-functions/hyperbolic-functions.html
def dsigmoid(y):
    return 1 - y ** 2


def makeMatrix(I, J, fill=0.0):
    m = []
    for i in range(I):
        m.append([fill] * J)
    return m


def randomizeMatrix(matrix, a, b):
    for i in range(len(matrix)):
        for j in range(len(matrix[0])):
            matrix[i][j] = random.uniform(a, b)




def backProp(dat_backProp, inp_start, inp_end, inc, out_max):


 #   start = time.time()

    myNN2 = NN(1, 5, 1)
    myNN2.train(dat_backProp)
    print(' ')

    inp2 = []
    out2 = []

    x1 = np.arange(inp_start, inp_end, inc)
    x = x1.reshape(-1, 1).tolist()

    for i in x:
        inp2.append(i)
        k =[]
        k.append( i[0]/inp_end )
        y = myNN2.fun(k)
        for j in range(len(y)):
            y[j] *= out_max
        out2.append(list(y))

 #   print(time.time() - start)

    plt.plot(np.asarray(inp2), np.asarray(out2), 'b--', x1, slotline_plot.f2(x1, 16), 'go')

    plt.ylabel('Impedance')
    plt.xlabel('W/d')
    plt.title('slotline using BackPropagation')
    plt.show()
    errorplot = abs(np.asarray(out2).T - slotline_plot.f2(x1, 16))/slotline_plot.f2(x1, 16)*100

    max_error = max(errorplot[0])
    print('BACK_PROP: ')
    print("max error: ",max_error)

    avg_error = 0
    for i in errorplot[0]:
        avg_error += i
    avg_error = avg_error/len(errorplot[0])

    print("average erro: ",avg_error)

    plt.plot(x1, errorplot[0], 'r--')
    plt.ylabel('absolute error')
    plt.xlabel('W/d')
    plt.title('%error using BackPropagation')
    plt.show()


    x = [0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]
    for i in x:
        ou = myNN2.fun([i / 0.995])
        print(ou[0] * 246.954)


def inBuilt(inp_dat,out_dat, inp_start, inp_end, inc, out_max):

    inp = []
    out = []
    for n in inp_dat:
        inp.append([int(n[0]*10)])
    for n in out_dat:
        out.append(int(n*100))



    classifiers = [
        ("LINEAR: ", linear_model.LinearRegression()),
        ('LOG-LBFGS: ', LogisticRegression(solver='lbfgs', max_iter=1000)),
        ('LOG-NEWTON: ', LogisticRegression(solver='newton-cg', max_iter=1000)),

        ('MLPCLAS-ADAM: ', MLPClassifier(solver='adam', max_iter=5000)),
        ('SGDREG: ', MLPRegressor(solver='lbfgs', max_iter=1000)),

    ]
    clas = [

        ('SVC', LinearSVC(max_iter=2000)),

    ]

    for name, clf in classifiers:
        print(' ')
        clf.fit(inp,out)
        print(name,': ')
        inp2 = []
        out2 = []

        x1 = np.arange(inp_start*10, inp_end*10, inc*10)
        x = x1.reshape(-1, 1).tolist()

        for i in x:
            inp2.append(i[0]/10)
            k = []
            k.append(i[0])
            y = clf.predict(k[0])/100 - 10 #-10 is -10
           # for j in range(len(y)):
           #     y[j] *= out_max
            out2.append(list(y))

            #   print(time.time() - start)

        plt.plot(np.asarray(inp2), np.asarray(out2), 'b--', np.asarray(inp2), slotline_plot.f2(np.asarray(inp2), 16), 'go')
        plt.ylabel('Impedance')
        plt.xlabel('w/h')
        plt.title('slotline using ' + name)
        plt.show()
        errorplot = abs(np.asarray(out2).T - slotline_plot.f2(np.asarray(inp2), 16))/slotline_plot.f2(np.asarray(inp2), 16)*100

        max_error = max(errorplot[0])
        print("max error: ", max_error)

        avg_error = 0
        for i in errorplot[0]:
            avg_error += i
        avg_error = avg_error / len(errorplot[0])

        print("average erro: ", avg_error)

        plt.plot(np.asarray(inp2), errorplot[0], 'r--')
        plt.ylabel('absolute error')

        plt.xlabel('W/d')
        plt.title('%error using '+name)
        plt.show()
        x = [0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]
        for i in x:
            ou = clf.predict(i * 10)
            print(ou[0] / 100)

def main():
    
    slotline_plot.plot()
    inp_dat = [
        [0.20000000000000001], [0.20500000000000002], [0.21000000000000002], [0.21500000000000002], [0.22000000000000003], [0.22500000000000003], [0.23000000000000004], [0.23500000000000004], [0.24000000000000005], [0.24500000000000005], [0.25000000000000006], [0.25500000000000006], [0.26000000000000006], [0.26500000000000007], [0.27000000000000007], [0.27500000000000008], [0.28000000000000008], [0.28500000000000009], [0.29000000000000009], [0.2950000000000001], [0.3000000000000001], [0.3050000000000001], [0.31000000000000011], [0.31500000000000011], [0.32000000000000012], [0.32500000000000012], [0.33000000000000013], [0.33500000000000013], [0.34000000000000014], [0.34500000000000014], [0.35000000000000014], [0.35500000000000015], [0.36000000000000015], [0.36500000000000016], [0.37000000000000016], [0.37500000000000017], [0.38000000000000017], [0.38500000000000018], [0.39000000000000018], [0.39500000000000018], [0.40000000000000019], [0.40500000000000019], [0.4100000000000002], [0.4150000000000002], [0.42000000000000021], [0.42500000000000021], [0.43000000000000022], [0.43500000000000022], [0.44000000000000022], [0.44500000000000023], [0.45000000000000023], [0.45500000000000024], [0.46000000000000024], [0.46500000000000025], [0.47000000000000025], [0.47500000000000026], [0.48000000000000026], [0.48500000000000026], [0.49000000000000027], [0.49500000000000027], [0.50000000000000022], [0.50500000000000034], [0.51000000000000023], [0.51500000000000035], [0.52000000000000024], [0.52500000000000036], [0.53000000000000025], [0.53500000000000036], [0.54000000000000026], [0.54500000000000037], [0.55000000000000027], [0.55500000000000038], [0.56000000000000028], [0.56500000000000039], [0.57000000000000028], [0.5750000000000004], [0.58000000000000029], [0.58500000000000041], [0.5900000000000003], [0.59500000000000042], [0.60000000000000031], [0.60500000000000043], [0.61000000000000032], [0.61500000000000044], [0.62000000000000033], [0.62500000000000044], [0.63000000000000034], [0.63500000000000045], [0.64000000000000035], [0.64500000000000046], [0.65000000000000036], [0.65500000000000047], [0.66000000000000036], [0.66500000000000048], [0.67000000000000037], [0.67500000000000049], [0.68000000000000038], [0.6850000000000005], [0.69000000000000039], [0.69500000000000051], [0.7000000000000004], [0.70500000000000052], [0.71000000000000041], [0.71500000000000052], [0.72000000000000042], [0.72500000000000053], [0.73000000000000043], [0.73500000000000054], [0.74000000000000044], [0.74500000000000055], [0.75000000000000044], [0.75500000000000056], [0.76000000000000045], [0.76500000000000057], [0.77000000000000046], [0.77500000000000058], [0.78000000000000047], [0.78500000000000059], [0.79000000000000048], [0.7950000000000006], [0.80000000000000049], [0.8050000000000006], [0.8100000000000005], [0.81500000000000061], [0.82000000000000051], [0.82500000000000062], [0.83000000000000052], [0.83500000000000063], [0.84000000000000052], [0.84500000000000064], [0.85000000000000053], [0.85500000000000065], [0.86000000000000054], [0.86500000000000066], [0.87000000000000055], [0.87500000000000067], [0.88000000000000056], [0.88500000000000068], [0.89000000000000057], [0.89500000000000068], [0.90000000000000058], [0.90500000000000069], [0.91000000000000059], [0.9150000000000007], [0.9200000000000006], [0.92500000000000071], [0.9300000000000006], [0.93500000000000072], [0.94000000000000061], [0.94500000000000073], [0.95000000000000062], [0.95500000000000074], [0.96000000000000063], [0.96500000000000075], [0.97000000000000064], [0.97500000000000075], [0.98000000000000065], [0.98500000000000076], [0.99000000000000066], [0.99500000000000077]
               ]

    out_dat = [
        72.190067132968139, 72.840651380604129, 73.496258032310067, 74.156899615333401, 74.822588656921567, 75.493337684321915, 76.169159224781907, 76.850065805548923, 77.536069953870395, 78.227184196993704, 78.923421062166312, 79.624793076635569, 80.331312767648939, 81.042992662453798, 81.759845288297583, 82.4818831724277, 83.209118842091527, 83.941564824536528, 84.679233647010093, 85.422137836759589, 86.170289921032492, 86.923702427076222, 87.682387882138087, 88.446358813465622, 89.215627748306161, 89.990207213907155, 90.770109737515995, 91.555347846380101, 92.345934067746853, 93.141880928863714, 93.943200956978046, 94.749906679337343, 95.562010623188911, 96.379525315780228, 97.202463284358657, 98.030837056171663, 98.864659158466623, 99.703942118490971, 100.54869846349209, 101.39894072071741, 102.25468141741433, 103.11593308083029, 103.98270823821267, 104.85501941680889, 105.73287914386638, 106.61629994663251, 107.50529435235475, 108.39987488828044, 109.30005408165707, 110.20584445973199, 111.11725854975263, 112.0343088789664, 112.95700797462074, 113.88536836396301, 114.81940257424066, 115.75912313270108, 116.70454256659173, 117.65567340315992, 118.61252816965319, 119.5751193933188, 120.54345960140429, 121.51756132115705, 122.49743707982441, 123.4830994046539, 124.47456082289283, 125.4718338617887, 126.4749310485888, 127.48386491054069, 128.49864797489164, 129.51929276888922, 130.54581181978062, 131.5782176548135, 132.61652280123508, 133.66073978629288, 134.71088113723422, 135.76695938130666, 136.82898704575743, 137.8969766578341, 138.97094074478395, 140.05089183385451, 141.13684245229302, 142.22880512734713, 143.32679238626403, 144.43081675629131, 145.54089076467619, 146.65702693866632, 147.7792378055089, 148.90753589245145, 150.04193372674132, 151.18244383562597, 152.32907874635282, 153.48185098616926, 154.6407730823226, 155.80585756206045, 156.97711695263013, 158.15456378127902, 159.33821057525449, 160.52806986180408, 161.72415416817512, 162.92647602161506, 164.13504794937126, 165.3498824786912, 166.57099213682218, 167.79838945101173, 169.03208694850716, 170.27209715655604, 171.51843260240565, 172.77110581330339, 174.03012931649664, 175.295515639233, 176.56727730875974, 177.84542685232427, 179.12997679717401, 180.42093967055644, 181.71832799971884, 183.02215431190879, 184.33243113437354, 185.64917099436062, 186.97238641911736, 188.30208993589122, 189.63829407192955, 190.9810113544799, 192.33025431078948, 193.6860354681059, 195.04836735367641, 196.4172624947486, 197.79273341856964, 199.17479265238717, 200.56345272344842, 201.958726159001, 203.36062548629212, 204.76916323256933, 206.18435192507994, 207.60620409107148, 209.03473225779123, 210.46994895248673, 211.91186670240523, 213.36049803479432, 214.81585547690128, 216.2779515559736, 217.74679879925867, 219.22240973400395, 220.70479688745667, 222.19397278686444, 223.68994995947457, 225.19274093253455, 226.70235823329168, 228.21881438899345, 229.74212192688725, 231.27229337422051, 232.80934125824058, 234.35327810619501, 235.90411644533106, 237.46186880289619, 239.02654770613779, 240.59816568230337, 242.17673525864029, 243.76226896239592, 245.35477932081758, 246.95427886115297
               ]

    dat_backProp = [
        [[0.2 / 0.995], [72.190067133 / 246.954278861]],
        [[0.205 / 0.995], [72.8406513806 / 246.954278861]],
        [[0.21 / 0.995], [73.4962580323 / 246.954278861]],
        [[0.215 / 0.995], [74.1568996153 / 246.954278861]],
        [[0.22 / 0.995], [74.8225886569 / 246.954278861]],
        [[0.225 / 0.995], [75.4933376843 / 246.954278861]],
        [[0.23 / 0.995], [76.1691592248 / 246.954278861]],
        [[0.235 / 0.995], [76.8500658055 / 246.954278861]],
        [[0.24 / 0.995], [77.5360699539 / 246.954278861]],
        [[0.245 / 0.995], [78.227184197 / 246.954278861]],
        [[0.25 / 0.995], [78.9234210622 / 246.954278861]],
        [[0.255 / 0.995], [79.6247930766 / 246.954278861]],
        [[0.26 / 0.995], [80.3313127676 / 246.954278861]],
        [[0.265 / 0.995], [81.0429926625 / 246.954278861]],
        [[0.27 / 0.995], [81.7598452883 / 246.954278861]],
        [[0.275 / 0.995], [82.4818831724 / 246.954278861]],
        [[0.28 / 0.995], [83.2091188421 / 246.954278861]],
        [[0.285 / 0.995], [83.9415648245 / 246.954278861]],
        [[0.29 / 0.995], [84.679233647 / 246.954278861]],
        [[0.295 / 0.995], [85.4221378368 / 246.954278861]],
        [[0.3 / 0.995], [86.170289921 / 246.954278861]],
        [[0.305 / 0.995], [86.9237024271 / 246.954278861]],
        [[0.31 / 0.995], [87.6823878821 / 246.954278861]],
        [[0.315 / 0.995], [88.4463588135 / 246.954278861]],
        [[0.32 / 0.995], [89.2156277483 / 246.954278861]],
        [[0.325 / 0.995], [89.9902072139 / 246.954278861]],
        [[0.33 / 0.995], [90.7701097375 / 246.954278861]],
        [[0.335 / 0.995], [91.5553478464 / 246.954278861]],
        [[0.34 / 0.995], [92.3459340677 / 246.954278861]],
        [[0.345 / 0.995], [93.1418809289 / 246.954278861]],
        [[0.35 / 0.995], [93.943200957 / 246.954278861]],
        [[0.355 / 0.995], [94.7499066793 / 246.954278861]],
        [[0.36 / 0.995], [95.5620106232 / 246.954278861]],
        [[0.365 / 0.995], [96.3795253158 / 246.954278861]],
        [[0.37 / 0.995], [97.2024632844 / 246.954278861]],
        [[0.375 / 0.995], [98.0308370562 / 246.954278861]],
        [[0.38 / 0.995], [98.8646591585 / 246.954278861]],
        [[0.385 / 0.995], [99.7039421185 / 246.954278861]],
        [[0.39 / 0.995], [100.548698463 / 246.954278861]],
        [[0.395 / 0.995], [101.398940721 / 246.954278861]],
        [[0.4 / 0.995], [102.254681417 / 246.954278861]],
        [[0.405 / 0.995], [103.115933081 / 246.954278861]],
        [[0.41 / 0.995], [103.982708238 / 246.954278861]],
        [[0.415 / 0.995], [104.855019417 / 246.954278861]],
        [[0.42 / 0.995], [105.732879144 / 246.954278861]],
        [[0.425 / 0.995], [106.616299947 / 246.954278861]],
        [[0.43 / 0.995], [107.505294352 / 246.954278861]],
        [[0.435 / 0.995], [108.399874888 / 246.954278861]],
        [[0.44 / 0.995], [109.300054082 / 246.954278861]],
        [[0.445 / 0.995], [110.20584446 / 246.954278861]],
        [[0.45 / 0.995], [111.11725855 / 246.954278861]],
        [[0.455 / 0.995], [112.034308879 / 246.954278861]],
        [[0.46 / 0.995], [112.957007975 / 246.954278861]],
        [[0.465 / 0.995], [113.885368364 / 246.954278861]],
        [[0.47 / 0.995], [114.819402574 / 246.954278861]],
        [[0.475 / 0.995], [115.759123133 / 246.954278861]],
        [[0.48 / 0.995], [116.704542567 / 246.954278861]],
        [[0.485 / 0.995], [117.655673403 / 246.954278861]],
        [[0.49 / 0.995], [118.61252817 / 246.954278861]],
        [[0.495 / 0.995], [119.575119393 / 246.954278861]],
        [[0.5 / 0.995], [120.543459601 / 246.954278861]],
        [[0.505 / 0.995], [121.517561321 / 246.954278861]],
        [[0.51 / 0.995], [122.49743708 / 246.954278861]],
        [[0.515 / 0.995], [123.483099405 / 246.954278861]],
        [[0.52 / 0.995], [124.474560823 / 246.954278861]],
        [[0.525 / 0.995], [125.471833862 / 246.954278861]],
        [[0.53 / 0.995], [126.474931049 / 246.954278861]],
        [[0.535 / 0.995], [127.483864911 / 246.954278861]],
        [[0.54 / 0.995], [128.498647975 / 246.954278861]],
        [[0.545 / 0.995], [129.519292769 / 246.954278861]],
        [[0.55 / 0.995], [130.54581182 / 246.954278861]],
        [[0.555 / 0.995], [131.578217655 / 246.954278861]],
        [[0.56 / 0.995], [132.616522801 / 246.954278861]],
        [[0.565 / 0.995], [133.660739786 / 246.954278861]],
        [[0.57 / 0.995], [134.710881137 / 246.954278861]],
        [[0.575 / 0.995], [135.766959381 / 246.954278861]],
        [[0.58 / 0.995], [136.828987046 / 246.954278861]],
        [[0.585 / 0.995], [137.896976658 / 246.954278861]],
        [[0.59 / 0.995], [138.970940745 / 246.954278861]],
        [[0.595 / 0.995], [140.050891834 / 246.954278861]],
        [[0.6 / 0.995], [141.136842452 / 246.954278861]],
        [[0.605 / 0.995], [142.228805127 / 246.954278861]],
        [[0.61 / 0.995], [143.326792386 / 246.954278861]],
        [[0.615 / 0.995], [144.430816756 / 246.954278861]],
        [[0.62 / 0.995], [145.540890765 / 246.954278861]],
        [[0.625 / 0.995], [146.657026939 / 246.954278861]],
        [[0.63 / 0.995], [147.779237806 / 246.954278861]],
        [[0.635 / 0.995], [148.907535892 / 246.954278861]],
        [[0.64 / 0.995], [150.041933727 / 246.954278861]],
        [[0.645 / 0.995], [151.182443836 / 246.954278861]],
        [[0.65 / 0.995], [152.329078746 / 246.954278861]],
        [[0.655 / 0.995], [153.481850986 / 246.954278861]],
        [[0.66 / 0.995], [154.640773082 / 246.954278861]],
        [[0.665 / 0.995], [155.805857562 / 246.954278861]],
        [[0.67 / 0.995], [156.977116953 / 246.954278861]],
        [[0.675 / 0.995], [158.154563781 / 246.954278861]],
        [[0.68 / 0.995], [159.338210575 / 246.954278861]],
        [[0.685 / 0.995], [160.528069862 / 246.954278861]],
        [[0.69 / 0.995], [161.724154168 / 246.954278861]],
        [[0.695 / 0.995], [162.926476022 / 246.954278861]],
        [[0.7 / 0.995], [164.135047949 / 246.954278861]],
        [[0.705 / 0.995], [165.349882479 / 246.954278861]],
        [[0.71 / 0.995], [166.570992137 / 246.954278861]],
        [[0.715 / 0.995], [167.798389451 / 246.954278861]],
        [[0.72 / 0.995], [169.032086949 / 246.954278861]],
        [[0.725 / 0.995], [170.272097157 / 246.954278861]],
        [[0.73 / 0.995], [171.518432602 / 246.954278861]],
        [[0.735 / 0.995], [172.771105813 / 246.954278861]],
        [[0.74 / 0.995], [174.030129316 / 246.954278861]],
        [[0.745 / 0.995], [175.295515639 / 246.954278861]],
        [[0.75 / 0.995], [176.567277309 / 246.954278861]],
        [[0.755 / 0.995], [177.845426852 / 246.954278861]],
        [[0.76 / 0.995], [179.129976797 / 246.954278861]],
        [[0.765 / 0.995], [180.420939671 / 246.954278861]],
        [[0.77 / 0.995], [181.718328 / 246.954278861]],
        [[0.775 / 0.995], [183.022154312 / 246.954278861]],
        [[0.78 / 0.995], [184.332431134 / 246.954278861]],
        [[0.785 / 0.995], [185.649170994 / 246.954278861]],
        [[0.79 / 0.995], [186.972386419 / 246.954278861]],
        [[0.795 / 0.995], [188.302089936 / 246.954278861]],
        [[0.8 / 0.995], [189.638294072 / 246.954278861]],
        [[0.805 / 0.995], [190.981011354 / 246.954278861]],
        [[0.81 / 0.995], [192.330254311 / 246.954278861]],
        [[0.815 / 0.995], [193.686035468 / 246.954278861]],
        [[0.82 / 0.995], [195.048367354 / 246.954278861]],
        [[0.825 / 0.995], [196.417262495 / 246.954278861]],
        [[0.83 / 0.995], [197.792733419 / 246.954278861]],
        [[0.835 / 0.995], [199.174792652 / 246.954278861]],
        [[0.84 / 0.995], [200.563452723 / 246.954278861]],
        [[0.845 / 0.995], [201.958726159 / 246.954278861]],
        [[0.85 / 0.995], [203.360625486 / 246.954278861]],
        [[0.855 / 0.995], [204.769163233 / 246.954278861]],
        [[0.86 / 0.995], [206.184351925 / 246.954278861]],
        [[0.865 / 0.995], [207.606204091 / 246.954278861]],
        [[0.87 / 0.995], [209.034732258 / 246.954278861]],
        [[0.875 / 0.995], [210.469948952 / 246.954278861]],
        [[0.88 / 0.995], [211.911866702 / 246.954278861]],
        [[0.885 / 0.995], [213.360498035 / 246.954278861]],
        [[0.89 / 0.995], [214.815855477 / 246.954278861]],
        [[0.895 / 0.995], [216.277951556 / 246.954278861]],
        [[0.9 / 0.995], [217.746798799 / 246.954278861]],
        [[0.905 / 0.995], [219.222409734 / 246.954278861]],
        [[0.91 / 0.995], [220.704796887 / 246.954278861]],
        [[0.915 / 0.995], [222.193972787 / 246.954278861]],
        [[0.92 / 0.995], [223.689949959 / 246.954278861]],
        [[0.925 / 0.995], [225.192740933 / 246.954278861]],
        [[0.93 / 0.995], [226.702358233 / 246.954278861]],
        [[0.935 / 0.995], [228.218814389 / 246.954278861]],
        [[0.94 / 0.995], [229.742121927 / 246.954278861]],
        [[0.945 / 0.995], [231.272293374 / 246.954278861]],
        [[0.95 / 0.995], [232.809341258 / 246.954278861]],
        [[0.955 / 0.995], [234.353278106 / 246.954278861]],
        [[0.96 / 0.995], [235.904116445 / 246.954278861]],
        [[0.965 / 0.995], [237.461868803 / 246.954278861]],
        [[0.97 / 0.995], [239.026547706 / 246.954278861]],
        [[0.975 / 0.995], [240.598165682 / 246.954278861]],
        [[0.98 / 0.995], [242.176735259 / 246.954278861]],
        [[0.985 / 0.995], [243.762268962 / 246.954278861]],
        [[0.99 / 0.995], [245.354779321 / 246.954278861]],
        [[0.995 / 0.995], [246.954278861 / 246.954278861]]
    ]  ##100
    inp_start = 0.2
    inp_end = 0.995
    inc = 0.005
    out_max = 246.954278861

    backProp(dat_backProp, inp_start, inp_end, inc, out_max)

    inBuilt(inp_dat,out_dat, inp_start, inp_end, inc, out_max)




if __name__ == "__main__":
    main()
